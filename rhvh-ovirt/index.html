<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>RHVH Ovirt - Everyday Linux</title>
<meta name="author" content="Everyday Linux">
<meta name="description" content="Examples of working solutions in everyday Linux.">

<meta name="generator" content="Hugo 0.55.6" />


<link href="//fonts.googleapis.com/css?family=Roboto:300" rel="stylesheet">
<link rel="stylesheet" href='/assets/css/main.6c6783c882af.css'>


<link rel="apple-touch-icon-precomposed" sizes="144x144" href="/assets/img/apple-touch-icon.png">
<link rel="shortcut icon" href="/assets/img/favicon.ico">


<link href="/posts/index.xml" rel="alternate" type="application/rss+xml" title="Everyday Linux" />
<link href="/posts/index.xml" rel="feed" type="application/rss+xml" title="Everyday Linux" />

<meta property="og:title" content="RHVH Ovirt" />
<meta property="og:description" content="RHVH Ovirt Red Hat documentation is pretty thorough on the subject of multipath, which is a lovely failover mechanism for connections between remote volumes provided by SANs and the host.[1][2]
 There is even this KCS that discusses the matter a little more: Internal disk device is being detected as multipath device
 In setting up my bare metal servers for RHVH, after adding all my disks to the system as basically single RAID 0 disks, I found that the base image for RHVH had particular settings for creating a single path multipath entry for these." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://briantward.github.io/rhvh-ovirt/" />
<meta property="article:published_time" content="2020-02-01T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2020-02-01T00:00:00&#43;00:00"/>



<meta itemprop="name" content="RHVH Ovirt">
<meta itemprop="description" content="RHVH Ovirt Red Hat documentation is pretty thorough on the subject of multipath, which is a lovely failover mechanism for connections between remote volumes provided by SANs and the host.[1][2]
 There is even this KCS that discusses the matter a little more: Internal disk device is being detected as multipath device
 In setting up my bare metal servers for RHVH, after adding all my disks to the system as basically single RAID 0 disks, I found that the base image for RHVH had particular settings for creating a single path multipath entry for these.">


<meta itemprop="datePublished" content="2020-02-01T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2020-02-01T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="1158">



<meta itemprop="keywords" content="multipath," />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="RHVH Ovirt"/>
<meta name="twitter:description" content="RHVH Ovirt Red Hat documentation is pretty thorough on the subject of multipath, which is a lovely failover mechanism for connections between remote volumes provided by SANs and the host.[1][2]
 There is even this KCS that discusses the matter a little more: Internal disk device is being detected as multipath device
 In setting up my bare metal servers for RHVH, after adding all my disks to the system as basically single RAID 0 disks, I found that the base image for RHVH had particular settings for creating a single path multipath entry for these."/>


  </head>
  <body>
    <nav>
  <div class="title">
    
      <a href="/" title="Homepage">
        Everyday Linux
      </a>
    
  </div>
  <div class="nav">
    
      <a href="/" title="Homepage">
        <svg
          xmlns="http://www.w3.org/2000/svg"
          width="24"
          height="24"
          viewBox="0 0 24 24"
          fill="none"
          stroke="currentColor"
          stroke-width="2"
          stroke-linecap="round"
          stroke-linejoin="round"
        >
          <path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z" />
          <polyline points="9 22 9 12 15 12 15 22" />
        </svg>
      </a>
    
    <a href="/posts/index.xml" title="RSS">
      <svg
        xmlns="http://www.w3.org/2000/svg"
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
        stroke="currentColor"
        stroke-width="2"
        stroke-linecap="round"
        stroke-linejoin="round"
        class="rss"
      >
        <path d="M4 11a9 9 0 0 1 9 9" />
        <path d="M4 4a16 16 0 0 1 16 16" />
        <circle cx="5" cy="19" r="1" />
      </svg>
    </a>
  </div>
</nav>

    <main>
      
  <article>
    <header>
      <p>
        <time datetime="2020-02-01 12:00">2020-02-01</time> &bull;
          
            
            <a href="/categories/rhvh">RHVH</a>
          
            , 
            <a href="/categories/ovirt">OVIRT</a>
          
            , 
            <a href="/categories/linux">LINUX</a>
          </p>
      <h1>RHVH Ovirt</h1>
      <p>
        
          
            
            <a href="/tags/multipath">
              <span class="hash">#</span>multipath</a>
          
        
      </p>
    </header>
    <section>
      
<div class="sect1">
<h2 id="_rhvh_ovirt">RHVH Ovirt</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Red Hat documentation is pretty thorough on the subject of multipath, which is a lovely failover mechanism for connections between remote volumes provided by SANs and the host.<sup class="footnote">[<a id="_footnoteref_1" class="footnote" href="#_footnotedef_1" title="View footnote.">1</a>]</sup><sup class="footnote">[<a id="_footnoteref_2" class="footnote" href="#_footnotedef_2" title="View footnote.">2</a>]</sup></p>
</div>
<div class="paragraph">
<p>There is even this KCS that discusses the matter a little more: <a href="https://access.redhat.com/solutions/66569">Internal disk device is being detected as multipath device</a></p>
</div>
<div class="paragraph">
<p>In setting up my bare metal servers for RHVH, after adding all my disks to the system as basically single RAID 0 disks, I found that the base image for RHVH had particular settings for creating a single path multipath entry for these.  I guess I could have just used the multipath device names and created filesystems from those, but it seems a bit odd to me.  Reading around in the docs suggests that if you have one path you shouldn&#8217;t use multipath at all, which kinda seems obvious.  I&#8217;ll be using a cloud replication strategy, so local disks in RAID 0 seems sufficient for this use case. As you can see, my root filesystem is on another disk as well.  If these fail and this host fails, I have other systems available.  This is also my home lab and I don&#8217;t care about rebuild/down time if one of these disks goes out.</p>
</div>
<div class="paragraph">
<p>So how do we fix this?</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>[root@host-a ~]# lsblk
NAME                                       MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT
sda                                          8:0    0  931G  0 disk
└─36842b2b04ff9d90025ab58e52a9663bc        253:7    0  931G  0 mpath
sdb                                          8:16   0  931G  0 disk
└─36842b2b04ff9d90025ab90d07fd7775b        253:9    0  931G  0 mpath
sdc                                          8:32   0  931G  0 disk
└─36842b2b04ff9d90025ab90f78228beb1        253:10   0  931G  0 mpath
sdd                                          8:48   0  931G  0 disk
└─36842b2b04ff9d90025ab910f8391d6c7        253:5    0  931G  0 mpath
sde                                          8:64   0  931G  0 disk
└─36842b2b04ff9d90025abddda04ad4d15        253:8    0  931G  0 mpath
sdf                                          8:80   0  931G  0 disk
└─36842b2b04ff9d90025abddf306245003        253:6    0  931G  0 mpath
sdg                                          8:96   1 59.6G  0 disk
├─sdg1                                       8:97   1    1G  0 part  /boot
└─sdg2                                       8:98   1 58.6G  0 part
  ├─rhvh-pool00_tmeta                      253:0    0    1G  0 lvm
  │ └─rhvh-pool00-tpool                    253:2    0 40.9G  0 lvm
  │   ├─rhvh-rhvh--4.3.7.1--0.20191211.0+1 253:3    0 13.9G  0 lvm   /
  │   ├─rhvh-pool00                        253:11   0 40.9G  0 lvm
  │   ├─rhvh-var_log_audit                 253:12   0    2G  0 lvm   /var/log/audit
  │   ├─rhvh-var_log                       253:13   0    8G  0 lvm   /var/log
  │   ├─rhvh-var                           253:14   0   15G  0 lvm   /var
  │   ├─rhvh-tmp                           253:15   0    1G  0 lvm   /tmp
  │   ├─rhvh-home                          253:16   0    1G  0 lvm   /home
  │   └─rhvh-var_crash                     253:17   0   10G  0 lvm   /var/crash
  ├─rhvh-pool00_tdata                      253:1    0 40.9G  0 lvm
  │ └─rhvh-pool00-tpool                    253:2    0 40.9G  0 lvm
  │   ├─rhvh-rhvh--4.3.7.1--0.20191211.0+1 253:3    0 13.9G  0 lvm   /
  │   ├─rhvh-pool00                        253:11   0 40.9G  0 lvm
  │   ├─rhvh-var_log_audit                 253:12   0    2G  0 lvm   /var/log/audit
  │   ├─rhvh-var_log                       253:13   0    8G  0 lvm   /var/log
  │   ├─rhvh-var                           253:14   0   15G  0 lvm   /var
  │   ├─rhvh-tmp                           253:15   0    1G  0 lvm   /tmp
  │   ├─rhvh-home                          253:16   0    1G  0 lvm   /home
  │   └─rhvh-var_crash                     253:17   0   10G  0 lvm   /var/crash
  └─rhvh-swap                              253:4    0    6G  0 lvm   [SWAP]</code></pre>
</div>
</div>
<div class="paragraph">
<p>Reading this statement, "If you have previously created a multipath device without using the find_multipaths parameter and then you later set the parameter to yes, you may need to remove the WWIDs of any device you do not want created as a multipath device from the /etc/multipath/wwids file."<sup class="footnote">[<a id="_footnoteref_3" class="footnote" href="#_footnotedef_3" title="View footnote.">3</a>]</sup> we come to understand that since we have only one path to each of these devices, we can set <code>find_multipaths yes</code> in our configuration file while also removing the stored <code>wwid</code>.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s back up the <code>wwid</code> file where these are stored.  Then remove the entries corresponding to the drives we want to remove from <code>multipath</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>[root@host-a ~]# cp /etc/multipath/wwids{,.bk}

[root@host-a ~]# cat /etc/multipath/wwids.bk
# Multipath wwids, Version : 1.0
# NOTE: This file is automatically maintained by multipath and multipathd.
# You should not need to edit this file in normal circumstances.
#
# Valid WWIDs:
/36842b2b04ff9d90025ab912284b3a47e/
/36842b2b04ff9d90025ab913085928cb2/
/36842b2b04ff9d90025ab90d07fd7775b/
/36842b2b04ff9d90025ab58e52a9663bc/
/36842b2b04ff9d90025ab910f8391d6c7/
/36842b2b04ff9d90025ab90f78228beb1/
/36842b2b04ff9d90025abddda04ad4d15/
/36842b2b04ff9d90025abddf306245003/

[root@host-a ~]# cat /etc/multipath/wwids
# Multipath wwids, Version : 1.0
# NOTE: This file is automatically maintained by multipath and multipathd.
# You should not need to edit this file in normal circumstances.
#
# Valid WWIDs:
/36842b2b04ff9d90025ab912284b3a47e/
/36842b2b04ff9d90025ab913085928cb2/</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can also see these registered by <code>multipath</code> prior to reboot:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>[root@host-a ~]# multipath -ll
36842b2b04ff9d90025ab90d07fd7775b dm-9 DELL    ,PERC 6/i
size=931G features='1 queue_if_no_path' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=1 status=active
  `- 1:2:1:0 sdb 8:16 active ready running
36842b2b04ff9d90025ab910f8391d6c7 dm-5 DELL    ,PERC 6/i
size=931G features='1 queue_if_no_path' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=1 status=active
  `- 1:2:3:0 sdd 8:48 active ready running
36842b2b04ff9d90025abddda04ad4d15 dm-8 DELL    ,PERC 6/i
size=931G features='1 queue_if_no_path' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=1 status=active
  `- 1:2:4:0 sde 8:64 active ready running
36842b2b04ff9d90025abddf306245003 dm-6 DELL    ,PERC 6/i
size=931G features='1 queue_if_no_path' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=1 status=active
  `- 1:2:5:0 sdf 8:80 active ready running
36842b2b04ff9d90025ab90f78228beb1 dm-10 DELL    ,PERC 6/i
size=931G features='1 queue_if_no_path' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=1 status=active
  `- 1:2:2:0 sdc 8:32 active ready running
36842b2b04ff9d90025ab58e52a9663bc dm-7 DELL    ,PERC 6/i
size=931G features='1 queue_if_no_path' hwhandler='0' wp=rw
`-+- policy='service-time 0' prio=1 status=active
  `- 1:2:0:0 sda 8:0  active ready running</code></pre>
</div>
</div>
<div class="paragraph">
<p>Normally we would edit <code>/etc/multipath.conf</code> but the RHVH image has things configured a little differently, and we see that this file is managed by <code>vdsm</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code># head -8 /etc/multipath.conf
# VDSM REVISION 1.8

# This file is managed by vdsm.
#
# The recommended way to add configuration for your storage is to add a
# drop-in configuration file in "/etc/multipath/conf.d/&lt;mydevice&gt;.conf".
# Settings in drop-in configuration files override settings in this
# file.</code></pre>
</div>
</div>
<div class="paragraph">
<p>Instead, we can add our own <code>.conf</code> file to get picked up and parsed after <code>/etc/multipath.conf</code>.  Here we will change the settings as needed.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code># cat /etc/multipath/conf.d/my.conf
defaults {
        find_multipaths yes
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now after a reboot we can see the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>[root@host-a ~]# lsblk
NAME                                                     MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda                                                        8:0    0  931G  0 disk
sdb                                                        8:16   0  931G  0 disk
sdc                                                        8:32   0  931G  0 disk
sdd                                                        8:48   0  931G  0 disk
sde                                                        8:64   0  931G  0 disk
sdf                                                        8:80   0  931G  0 disk
sdg                                                        8:96   1 59.6G  0 disk
├─sdg1                                                     8:97   1    1G  0 part /boot
└─sdg2                                                     8:98   1 58.6G  0 part
  ├─rhvh-pool00_tmeta                                    253:0    0    1G  0 lvm
  │ └─rhvh-pool00-tpool                                  253:2    0 40.9G  0 lvm
  │   ├─rhvh-rhvh--4.3.7.1--0.20191211.0+1               253:3    0 13.9G  0 lvm  /
  │   ├─rhvh-pool00                                      253:5    0 40.9G  0 lvm
  │   ├─rhvh-var_log_audit                               253:6    0    2G  0 lvm  /var/log/audit
  │   ├─rhvh-var_log                                     253:7    0    8G  0 lvm  /var/log
  │   ├─rhvh-var                                         253:8    0   15G  0 lvm  /var
  │   ├─rhvh-tmp                                         253:9    0    1G  0 lvm  /tmp
  │   ├─rhvh-home                                        253:10   0    1G  0 lvm  /home
  │   └─rhvh-var_crash                                   253:11   0   10G  0 lvm  /var/crash
  ├─rhvh-pool00_tdata                                    253:1    0 40.9G  0 lvm
  │ └─rhvh-pool00-tpool                                  253:2    0 40.9G  0 lvm
  │   ├─rhvh-rhvh--4.3.7.1--0.20191211.0+1               253:3    0 13.9G  0 lvm  /
  │   ├─rhvh-pool00                                      253:5    0 40.9G  0 lvm
  │   ├─rhvh-var_log_audit                               253:6    0    2G  0 lvm  /var/log/audit
  │   ├─rhvh-var_log                                     253:7    0    8G  0 lvm  /var/log
  │   ├─rhvh-var                                         253:8    0   15G  0 lvm  /var
  │   ├─rhvh-tmp                                         253:9    0    1G  0 lvm  /tmp
  │   ├─rhvh-home                                        253:10   0    1G  0 lvm  /home
  │   └─rhvh-var_crash                                   253:11   0   10G  0 lvm  /var/crash
  └─rhvh-swap                                            253:4    0    6G  0 lvm  [SWAP]

[root@host-a ~]# multipath -ll
[root@host-a ~]#</code></pre>
</div>
</div>
</div>
</div>
<div id="footnotes">
<hr>
<div class="footnote" id="_footnotedef_1">
<a href="#_footnoteref_1">1</a>. <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/dm_multipath/ignore_localdisk_procedure" class="bare">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/dm_multipath/ignore_localdisk_procedure</a>
</div>
<div class="footnote" id="_footnotedef_2">
<a href="#_footnoteref_2">2</a>. <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/dm_multipath/config_file_defaults" class="bare">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/dm_multipath/config_file_defaults</a>
</div>
<div class="footnote" id="_footnotedef_3">
<a href="#_footnoteref_3">3</a>. <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/dm_multipath/config_file_blacklist" class="bare">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/dm_multipath/config_file_blacklist</a>
</div>
</div>
</section>
    <footer>
      
      
    </footer>
  </article>

    </main>
    <footer>
  <div class="powered-by">
    Powered by <a href="https://gohugo.io" title="A Fast and Flexible Website Generator">Hugo</a> &amp; <a href="https://github.com/eshlox/simplicity" title="Hugo theme">Simplicity</a>.
  </div>
  <div class="copyright">
    &copy; 2020 Everyday Linux. <a href="http://creativecommons.org/licenses/by-sa/4.0/">Some Rights Reserved</a>.
  </div>
</footer>

    <script src="/assets/js/main.28b0c18ba028.js"></script>
    
    
  </body>
</html>

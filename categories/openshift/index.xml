<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>openshift on Everyday Linux</title>
    <link>https://briantward.github.io/categories/openshift/</link>
    <description>Recent content in openshift on Everyday Linux</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 10 Jun 2023 00:00:00 +0000</lastBuildDate>
    
      <atom:link href="https://briantward.github.io/categories/openshift/index.xml" rel="self" type="application/rss+xml" />
    
    
      <item>
        <title>Helm and ArgoCD</title>
        <link>https://briantward.github.io/helm-argocd/</link>
        <pubDate>Sat, 10 Jun 2023 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/helm-argocd/</guid>
        <description>Helm and ArgoCD Feature discussions…​
   https://argoproj.github.io/argo-cd/user-guide/helm/
  https://github.com/argoproj/argo-cd/issues/5826
  https://github.com/argoproj/argo-cd/issues/2789
     ArgoCD and useful GitOps blogs   https://github.com/sa-ne/acm-argocd-application
  https://github.com/open-gitops/documents/tree/main
  https://www.youtube.com/playlist?list=PLaR6Rq6Z4IqfGCkI28cUMbNhPhsnj4nq3
  https://tost.dev/blog/ignore_differences_in_argocd/
  https://argo-cd.readthedocs.io/en/stable/user-guide/compare-options/
  https://github.com/jannfis/argo-cd/pull/17/files
  https://github.com/argoproj/argo-cd/pull/9791
  https://github.com/argoproj/argo-cd/issues/8683#issuecomment-1111605207
  https://github.com/argoproj/argo-cd/issues/4487
  https://docs.openshift.com/container-platform/4.12/cicd/gitops/argo-cd-custom-resource-properties.html
  https://docs.openshift.com/container-platform/4.12/cicd/gitops/argo-cd-custom-resource-properties.html
  https://developers.redhat.com/articles/2022/04/13/manage-namespaces-multitenant-clusters-argo-cd-kustomize-and-helm
  https://cloud.redhat.com/blog/a-guide-to-going-from-zero-to-openshift-cluster-with-gitops
  https://cloud.redhat.com/blog/configuring-openshift-cluster-with-applicationsets-using-helmkustomize-and-acm-policies
  https://rcarrata.com/openshift/argo-and-acm/</description>
      </item>
    
      <item>
        <title>OpenShift SecurityContextConstraints (SCC)</title>
        <link>https://briantward.github.io/scc/</link>
        <pubDate>Sat, 10 Jun 2023 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/scc/</guid>
        <description>OpenShift SecurityContextConstraints (SCC) It’s recommended to use RBAC to provide access to existing SCCs.
 Using # oc adm policy add-scc-to-user anyuid -z useroot will create a rolebinding to a role granting access to modify the SCC (this was changed from directly adding users/groups to the SCC object itself in earlier versions of OpenShift). Note that the -n for namespace is not acknowledged correctly for this command at this time (2023-06-10, TODO: file an issue or fix the code), so you should be in the namespace of the serviceaccount to make this work correctly.</description>
      </item>
    
      <item>
        <title>ArgoCD MCO</title>
        <link>https://briantward.github.io/argocd-mco/</link>
        <pubDate>Sat, 22 Apr 2023 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/argocd-mco/</guid>
        <description>Provide ArgoCD permissions to control Advanced Cluster Management’s Multicluster Observability RHACM MCO One off command:
  $ oc adm policy add-cluster-role-to-user multiclusterobservabilities.observability.open-cluster-management.io-v1beta2-admin -z openshift-gitops-argocd-application-controller -n openshift-gitops clusterrole.rbac.authorization.k8s.io/multiclusterobservabilities.observability.open-cluster-management.io-v1beta2-admin added: &amp;#34;openshift-gitops-argocd-application-controller&amp;#34;    $ oc adm policy add-cluster-role-to-user multiclusterobservabilities.observability.open-cluster-management.io-v1beta1-admin -z openshift-gitops-argocd-application-controller -n openshift-gitops clusterrole.rbac.authorization.k8s.io/multiclusterobservabilities.observability.open-cluster-management.io-v1beta1-admin added: &amp;#34;openshift-gitops-argocd-application-controller&amp;#34;   Declarative Spec for the same:
 apiVersion: v1 items: - apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: argocd-mcov1-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: multiclusterobservabilities.</description>
      </item>
    
      <item>
        <title>Debugging Containers in OpenShift</title>
        <link>https://briantward.github.io/debugging-containers-in-openshift/</link>
        <pubDate>Sat, 22 Apr 2023 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/debugging-containers-in-openshift/</guid>
        <description>Debugging Containers in OpenShift   https://docs.openshift.com/container-platform/4.11/support/troubleshooting/investigating-pod-issues.html
  https://www.redhat.com/sysadmin/how-oc-debug-works
  https://gist.github.com/nmasse-itix/0173f925587155ce600a74fe6a912595
  https://developers.redhat.com/blog/2020/01/09/debugging-applications-within-red-hat-openshift-containers
  https://access.redhat.com/solutions/1611883
  https://briantward.github.io/nsenter/
  https://www.redhat.com/sysadmin/container-namespaces-nsenter
  https://access.redhat.com/solutions/4569211
   TODO add more thoughts on this topic
   </description>
      </item>
    
      <item>
        <title>Default ResourceQuota</title>
        <link>https://briantward.github.io/default-resourcequota/</link>
        <pubDate>Sat, 22 Apr 2023 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/default-resourcequota/</guid>
        <description>Default ResourceQuota The question of how do I manage application onboarding usually introduces the topic of how to set specific values for ResourceQuota for the applications being onboarded.
 In the past I have been a part of teams suggesting the concept of T-shirt sizing your applications, e.g. Small, Medium, and Large. Then, during onboarding process, one selects the appropriate T-shirt size ResourceQuota. Your values here could be dependent on your typical workloads.</description>
      </item>
    
      <item>
        <title>MetalLB Demo</title>
        <link>https://briantward.github.io/metallb-demo/</link>
        <pubDate>Sat, 22 Apr 2023 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/metallb-demo/</guid>
        <description>MetalLB Demo Adapted from other people’s work. This demo shows MetalLB spreading requests across two k8s clusters.
 https://cloud.redhat.com/blog/metallb-in-bgp-mode
 Update your linux OS settings to handle what is about to happen in the demo. Otherwise things will fail.
 $ sudo sysctl fs.inotify.max_user_watches=524288 $ sudo sysctl fs.inotify.max_user_instances=8192   Make that permanent:
 # echo &amp;#39;fs.inotify.max_user_watches=524288 fs.inotify.max_user_instances=8192&amp;#39; &amp;gt; /etc/sysctl.d/90-metallb-demo.conf   Install golang and a needed library. This was undocumented at the time.</description>
      </item>
    
      <item>
        <title>OpenShift Capacity Planning</title>
        <link>https://briantward.github.io/openshift-capacity-planning/</link>
        <pubDate>Sat, 22 Apr 2023 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-capacity-planning/</guid>
        <description>OpenShift Capacity Planning This is a great series of articles written by Raffaele Spazzoli.
   https://cloud.redhat.com/blog/full-cluster-capacity-management-monitoring-openshift
  https://cloud.redhat.com/blog/full-cluster-part-2-protecting-nodes
  https://cloud.redhat.com/blog/full-cluster-part-3-capacity-management
  https://cloud.redhat.com/blog/how-full-is-my-cluster-part-4-right-sizing-pods-with-vertical-pod-autoscaler
  https://cloud.redhat.com/blog/how-full-is-my-cluster-part-5-a-capacity-management-dashboard
   Subnet Sizing for OpenShift TODO cleanup formatting here
 ############################################################################# networking.clusterNetwork[].cidr default: 10.128.0.0/14 (262144 addresses) networking.clusterNetwork[].hostPrefix default: /23 (512 addresses) networking.serviceNetwork default: 172.30.0.0/16 (65536 addresses) networking.machineNetwork[].cidr default: 10.0.0.0/16 (65536 addresses) Each k8s node is assigned a /23 subnet (hostPrefix) out of the given CIDR, allowing for 510 (2^(32 - 23) - 2) Pod IP addresses.</description>
      </item>
    
      <item>
        <title>RHACM</title>
        <link>https://briantward.github.io/rhacm/</link>
        <pubDate>Sat, 22 Apr 2023 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/rhacm/</guid>
        <description>RHACM https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.4/html-single/applications/index#gitops-config
 ArgoCD GitOps for Configuration Management GitOps instance per ManagedClusterSet
 Things that make you pick single, central:
   Good starting point
  Simple management, single place
   Things that make you change to distributed:
   One ArgoCD is overloaded.
  Separation of duties. One ArgoCD for Production. One ArgoCD for NonProduction.
  Separation of use. E.G. PCI Compliance, separation of elements in different clusters</description>
      </item>
    
      <item>
        <title>Secret Management in Kubernetes</title>
        <link>https://briantward.github.io/secret-management-in-kubernetes/</link>
        <pubDate>Sat, 22 Apr 2023 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/secret-management-in-kubernetes/</guid>
        <description>Secret Management in Kubernetes   BeyondTrust
  SealedSecret
  Ansible Vault
  Hashicorp Vault
   TODO add links to resources. Add thoughts to objectives.
   </description>
      </item>
    
      <item>
        <title>OpenShift HA</title>
        <link>https://briantward.github.io/openshift-ha/</link>
        <pubDate>Sat, 22 Oct 2022 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-ha/</guid>
        <description>OpenShift HA Good references and resources
   https://www.openshift.com/blog/disaster-recovery-strategies-for-applications-running-on-openshift
  https://www.openshift.com/blog/deploying-openshift-applications-multiple-datacenters
  https://www.openshift.com/blog/stateful-workloads-and-the-two-data-center-conundrum
  https://www.openshift.com/blog/disaster-recovery-with-gitops
  https://docs.openshift.com/container-platform/4.6/backup_and_restore/disaster_recovery/about-disaster-recovery.html
  https://cloud.redhat.com/blog/8-application-design-principles-to-cope-with-openshift-maintenance-operations
  https://cloud.redhat.com/blog/stateful-workloads-and-the-two-data-center-conundrum
  https://cloud.redhat.com/blog/disaster-recovery-strategies-for-applications-running-on-openshift
  https://cloud.redhat.com/blog/deploying-openshift-applications-multiple-datacenters
  https://cloud.redhat.com/blog/geographically-distributed-stateful-workloads-part-two-cockroachdb
  https://cloud.redhat.com/blog/geographically-distributed-stateful-workloads-part-3-keycloak
  https://cloud.redhat.com/blog/geographically-distributed-stateful-workloads-part-four-kafka
  https://cloud.redhat.com/blog/geographically-distributed-stateful-workloads-part-five-yugabytedb
   DR options
 Failure Scenarios
   Complete datacenter outage
  Availability Zone failure (network segment or group of racks)</description>
      </item>
    
      <item>
        <title>Create Bastion in AWS to Reach OpenShift Over Private VPC</title>
        <link>https://briantward.github.io/aws-bastion-to-ocp-over-vpc/</link>
        <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/aws-bastion-to-ocp-over-vpc/</guid>
        <description>Create Bastion in AWS to Reach OpenShift Over Private VPC  vpc peering request from VPC-A
  vpc peering accept from VPC-B
  set route from VPC-A to VPC-B on all route definitions
  set route from VPC-B to VPC-A on all route definitions
  set security group allow on VPC-A sg’s to accpet connections from VPC-B
  set security group allow on VPC-B sg’s to accpet connections from VPC-A</description>
      </item>
    
      <item>
        <title>Kubernetes and OpenShift Certificate Signing Requests</title>
        <link>https://briantward.github.io/kubernetes-openshift-csr/</link>
        <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/kubernetes-openshift-csr/</guid>
        <description>Kubernetes and OpenShift Certificate Signing Requests Types of CSRs
 https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/
   kubernetes.io/kube-apiserver-client: signs certificates that will be honored as client certificates by the API server. Never auto-approved by kube-controller-manager.
  kubernetes.io/kube-apiserver-client-kubelet: signs client certificates that will be honored as client certificates by the API server. May be auto-approved by kube-controller-manager.
  kubernetes.io/kubelet-serving: signs serving certificates that are honored as a valid kubelet serving certificate by the API server, but has no other guarantees.</description>
      </item>
    
      <item>
        <title>OpenShift Automation Examples</title>
        <link>https://briantward.github.io/openshift-automation-examples/</link>
        <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-automation-examples/</guid>
        <description>OpenShift Automation Examples   https://github.com/sa-ne/openshift4-rhv-upi
  https://github.com/sushilsuresh/ocp4-ansible-roles
  https://github.com/redhat-cop/infra-ansible
  https://github.com/openshift/openshift-ansible
  https://github.com/openshift/installer/tree/master/upi/
     </description>
      </item>
    
      <item>
        <title>OpenShift Monitoring Blogs</title>
        <link>https://briantward.github.io/openshift-monitoring-blogs/</link>
        <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-monitoring-blogs/</guid>
        <description>OpenShift Monitoring Blogs   https://cloud.redhat.com/blog/monitoring-infrastructure-openshift-4.x-using-zabbix-operator
  https://cloud.redhat.com/blog/how-to-monitoring-openshift-4.x-with-zabbix-using-prometheus-part-2
  https://cloud.redhat.com/blog/prometheus-alerts-on-openshift
     </description>
      </item>
    
      <item>
        <title>OpenShift CICD</title>
        <link>https://briantward.github.io/openshift-cicd/</link>
        <pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-cicd/</guid>
        <description>OpenShift CICD   environment changes that progress through the environments
  application performance configuration
  feature flags
  environment changes specific to an environment
  connection endpoint URLs
  image name/tag
   Scenario 0: First project deployment - scaffold code, base image, build pipeline
 Scenario 1: Application is rebuilt from source code, base image doesn’t change: - build binary image - rebuild container image from same base image - tag with new version numbering scheme - (dev) picks up new version and deploys immediately</description>
      </item>
    
      <item>
        <title>OpenShift HA</title>
        <link>https://briantward.github.io/openshift-ha/</link>
        <pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-ha/</guid>
        <description>OpenShift HA Good references and resources
   https://www.openshift.com/blog/disaster-recovery-strategies-for-applications-running-on-openshift
  https://www.openshift.com/blog/deploying-openshift-applications-multiple-datacenters
  https://www.openshift.com/blog/stateful-workloads-and-the-two-data-center-conundrum
  https://www.openshift.com/blog/disaster-recovery-with-gitops
  https://docs.openshift.com/container-platform/4.6/backup_and_restore/disaster_recovery/about-disaster-recovery.html
   DR options
 Failure Scenarios
   Complete datacenter outage
  Availability Zone failure (network segment or group of racks)
  Rack failure
  Host failure
  VM failure
  OpenShift master failure
  OpenShift etcd failure (etcd backup and restore)</description>
      </item>
    
      <item>
        <title>OpenShift RBAC</title>
        <link>https://briantward.github.io/openshift-roles/</link>
        <pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-roles/</guid>
        <description>OpenShift RBAC oc auth can-i VERB OBJECT oc adm policy who-can VERB OBJECT   oc get clusterrole basic-user -o yaml oc get clusterrole cluster-admin -o yaml   https://www.openshift.com/blog/fine-grained-iam-roles-for-openshift-applications
   </description>
      </item>
    
      <item>
        <title>OpenShift SSH Client in a Pod</title>
        <link>https://briantward.github.io/openshift-ssh-client-in-pod/</link>
        <pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-ssh-client-in-pod/</guid>
        <description>OpenShift SSH Client in a Pod This is just conceptual.
 $ mkdir ssh-test &amp;amp;&amp;amp; cd ssh-test $ echo &amp;#39;FROM registry.access.redhat.com/rhel7:latest USER root RUN yum-config-manager --enable rhel-7-server-rpms &amp;amp;&amp;amp; yum install openssh-clients -y&amp;#39; &amp;gt; Dockerfile $ oc new-project ssh-test $ oc new-build . --to ssh-test $ oc start-build ssh-test --from-dir . $ oc run -i -t ssh-test --image=172.30.1.1:5000/ssh-test/ssh-test --command -- /bin/bash bash-4.2$ which ssh bash-4.2$ exit $ echo &amp;#39;&amp;lt;PRIVATE_KEY_FILE&amp;gt; id_rsa $ echo &amp;#39;&amp;lt;KNOWN_HOSTS_FILE&amp;#39; &amp;gt; known_hosts $ oc create secret generic id-key --from-file=id_rsa $ oc create secret generic known-hosts --from-file=known_hosts $ oc set volumes dc/ssh-test --add --name=id-key --type=secret --secret-name=id-key --mount-path=/ssh/id_rsa --default-mode=&amp;#39;0600&amp;#39; $ oc set volumes dc/ssh-test --add --name=known-hosts --type=secret --secret-name=known-hosts --mount-path=/ssh/known_hosts --default-mode=&amp;#39;0600&amp;#39; $ ssh -i /ssh/id_rsa/id_rsa user@remote.</description>
      </item>
    
      <item>
        <title>OpenShift Time and Date</title>
        <link>https://briantward.github.io/openshift4-timedate/</link>
        <pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift4-timedate/</guid>
        <description>OpenShift Time and Date OCP timedate
   https://access.redhat.com/solutions/2567961
  https://access.redhat.com/solutions/4994241
     </description>
      </item>
    
      <item>
        <title>Using nsenter strace and tcpdump in OpenShift containers</title>
        <link>https://briantward.github.io/nsenter/</link>
        <pubDate>Sat, 20 Feb 2021 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/nsenter/</guid>
        <description>Using nsenter strace and tcpdump in OpenShift containers   nsenter
  strace
  tcpdump
  oc rsyn -h
   capture a tcpdump of the base ethernet interface on the node:
 # tcpdump -s 0 -n -i ethX -w /tmp/$(hostname)-$(date +&amp;#34;%Y-%m-%d-%H-%M-%S&amp;#34;).pcap   find the container ID from the container engine:
 # docker ps # cid=&amp;lt;docker-container-id&amp;gt;   find the runtime PID from inspecting the container itself:</description>
      </item>
    
      <item>
        <title>Installing OpenShift Container Storage on IBM Cloud Bare Metal RedHat Openshift Kubernetes Service (ROKS)</title>
        <link>https://briantward.github.io/installing-ocs-on-ibm-cloud-bare-metal/</link>
        <pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/installing-ocs-on-ibm-cloud-bare-metal/</guid>
        <description>Installing OpenShift Container Storage on IBM Cloud Bare Metal RedHat Openshift Kubernetes Service (ROKS) This feature set, deploying OCS on Bare Metal, has not gone GA yet and is still a work in progress.[1] It should be coming soon, but the main issue is that there is no documentation specific to this deployment scenario and the kubelet home directory on IBM ROKS is not the same as Red Hat OpenShift Container Platform.</description>
      </item>
    
      <item>
        <title>New Relic on OpenShift 4</title>
        <link>https://briantward.github.io/new-relic-on-openshift-4/</link>
        <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/new-relic-on-openshift-4/</guid>
        <description>New Relic on OpenShift 4 Follow the instructions on New Relic’s website:
 https://docs.newrelic.com/docs/integrations/kubernetes-integration/link-your-applications/link-your-applications-kubernetes
 Modified here. There is no need for adding the MutatingAdmissionWebhook as this is already enabled by default in OCP 4, so skip that section.
  Download the YAML file:
curl -O http://download.newrelic.com/infrastructure_agent/integrations/kubernetes/k8s-metadata-injection-latest.yaml     Replace YOUR_CLUSTER_NAME with the name of your cluster in the YAML file.
  Apply the YAML file to your Kubernetes cluster:</description>
      </item>
    
      <item>
        <title>Installing RHMI on OpenShift</title>
        <link>https://briantward.github.io/installing-rhmi-on-openshift/</link>
        <pubDate>Sun, 15 Mar 2020 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/installing-rhmi-on-openshift/</guid>
        <description>Installing RHMI on OpenShift   </description>
      </item>
    
      <item>
        <title>Installing GlusterFS on OpenShift</title>
        <link>https://briantward.github.io/installing-glusterfs-on-openshift/</link>
        <pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/installing-glusterfs-on-openshift/</guid>
        <description>Installing GlusterFS on OpenShift OpenShift 3.11.170 OpenShift Container Storage 3.11.5
 Working through some errors, after following documentation for a normal OpenShift deploy with GlusterFS (OpenShift Container Storage)
 On my first run, I hit this for some odd reason:
 TASK [cockpit : Install cockpit-ws] ************************************************************************************************************** FAILED - RETRYING: Install cockpit-ws (3 retries left). FAILED - RETRYING: Install cockpit-ws (2 retries left). FAILED - RETRYING: Install cockpit-ws (1 retries left).</description>
      </item>
    
      <item>
        <title>CoreDNS Custom DNS Running on OpenShift</title>
        <link>https://briantward.github.io/coredns-nonprivileged/</link>
        <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/coredns-nonprivileged/</guid>
        <description>CoreDNS Custom DNS Running on OpenShift This example provides the bare requirements for deploying a custom DNS server on OpenShift using the default restricted SCC profile, which means that the pod is run without privileges as a nonroot user. This should work fine on both OpenShift 3.11 and 4.x.
 You can edit this dns-config ConfigMap as necessary to modify the DNS zone records as you need.
 $ echo &amp;#39;apiVersion: v1 data: Corefile: | example.</description>
      </item>
    
      <item>
        <title>An Introduction to Running Java on Kubernetes</title>
        <link>https://briantward.github.io/running-java-on-kubernetes/</link>
        <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/running-java-on-kubernetes/</guid>
        <description>An Introduction to Running Java on Kubernetes You will need access to a container build tool, a container runtime environment, and a kubernetes environment. There will be suggestions along the way for acquiring these. You will need a virtualization technology to run the minikube or minishift VMs for your kubernetes development environment.
   Build a Demo Spring Boot App  Build an app
$ mkdir -p my-java-app/container $ cd my-java-app $ git clone https://github.</description>
      </item>
    
      <item>
        <title>Create Root Access from any Build in OpenShift</title>
        <link>https://briantward.github.io/openshift-root-access-on-build/</link>
        <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-root-access-on-build/</guid>
        <description>Create Root Access from any Build in OpenShift TODO: split examples, show DC options
 This example adds an SCC permission to run root on the default service account, allowing you to run root containers. This is not recommended for normal practice and should only be done during troubleshooting, on an isolated nonprod worker node. You could also choose to create a service account specific to this one application and configure the DeploymentConfig to use that (this is not shown here).</description>
      </item>
    
      <item>
        <title>Docker Core Dumps</title>
        <link>https://briantward.github.io/docker-core-dump/</link>
        <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/docker-core-dump/</guid>
        <description>Docker Core Dumps  Get the container id (CID) of the pod to be checked. From the node running the pod. You will need to be able to identify your container ID.
# docker ps # CID=xxx     Check if docker is logging
# docker logs &amp;lt;CID&amp;gt;     Check node’s journal
# journalctl CONTAINER_ID=&amp;lt;CID&amp;gt;     Grab lsof and gcore for each process: dockerd-current, docker-containerd-current, and docker-containerd-shim-current.</description>
      </item>
    
      <item>
        <title>OpenShift Application Core Dumps</title>
        <link>https://briantward.github.io/openshift-application-coredumps/</link>
        <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-application-coredumps/</guid>
        <description>OpenShift Application Core Dumps When logs fail to provide us the information we need to diagnose an application problem, we may find it useful to take core dumps of memory, showing us the processes as they are currently running in the system. This is not something we want to do on a regular basis in production. Ideally such problems are discovered during application performance and load testing in lower environments. In reality we frequently find something unique about the real-world application load that our test scenarios could never uncover.</description>
      </item>
    
      <item>
        <title>OpenShift Firewall</title>
        <link>https://briantward.github.io/openshift-firewall/</link>
        <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-firewall/</guid>
        <description>OpenShift Firewall To add a firewall entry on 3.x, where the firewall installed was iptables rather than firewalld
 iptables -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT     </description>
      </item>
    
      <item>
        <title>OpenShift Router Quick Links</title>
        <link>https://briantward.github.io/openshift-router/</link>
        <pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-router/</guid>
        <description>OpenShift Router quick links router balancing
 router environment variables
   https://docs.openshift.com/container-platform/3.7/architecture/networking/routes.html#env-variables
  https://docs.openshift.com/container-platform/3.6/architecture/networking/routes.html#load-balancing
  https://docs.openshift.com/container-platform/3.11/architecture/networking/routes.html#route-specific-annotations
  https://docs.openshift.com/container-platform/3.7/architecture/networking/routes.html#routes-sticky-sessions
  https://docs.openshift.com/container-platform/3.7/dev_guide/routes.html#dev-guide-routes-allowing-endpoints-to-control-cookies
   passthrough
   https://github.com/openshift/origin/commit/a4815c6314f9df1d2ce8060216d0924181c48b6c Changed the router default to roundrobin if non-zero weights are used https://bugzilla.redhat.com/show_bug.cgi?id=1416869
   stick sessions in haproxy
   https://www.haproxy.com/blog/load-balancing-affinity-persistence-sticky-sessions-what-you-need-to-know/
  http://www.haproxy.org/download/1.8/doc/configuration.txt (3.11)
   kubernetes service
   https://kubernetes.io/docs/concepts/services-networking/service/</description>
      </item>
    
      <item>
        <title>Installing Integr8tly</title>
        <link>https://briantward.github.io/integr8tly-installation/</link>
        <pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/integr8tly-installation/</guid>
        <description>Installing Integr8tly TODO: clean up and sort out steps, using container installer
 git clone -b fix-verify-launcher https://github.com/briantward/installation.git oc login ${OCP_MASTER_URL} ansible.cfg id_rsa inventories/hosts ansible -m ping all ansible-playbook -i inventories/hosts playbooks/install.yml pip install jsonpointer     </description>
      </item>
    
      <item>
        <title>ElasticSearch on OpenShift</title>
        <link>https://briantward.github.io/elasticsearch/</link>
        <pubDate>Sun, 17 Nov 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/elasticsearch/</guid>
        <description>ElasticSearch on OpenShift   Node Tuning
$ sysctl -w vm.max_map_count=262144 $ echo &amp;#34;vm.max_map_count=262144&amp;#34; &amp;gt; /etc/sysctl.d/90-logging.conf   https://github.com/openshift/openshift-ansible/blob/release-3.11/playbooks/openshift-logging/private/config.yml#L94-L116
 https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html
   Get indices, run from inside container
curl --key /etc/elasticsearch/secret/admin-key --cert /etc/elasticsearch/secret/admin-cert --cacert /etc/elasticsearch/secret/admin-ca https://localhost:9200/_cat/indices -s     Delete red indices, run from inside container
for i in $(curl --key /etc/elasticsearch/secret/admin-key --cert /etc/elasticsearch/secret/admin-cert --cacert /etc/elasticsearch/secret/admin-ca https://localhost:9200/_cat/indices -s | grep red | awk &amp;#39;{print $3}&amp;#39;); do curl --key /etc/elasticsearch/secret/admin-key --cert /etc/elasticsearch/secret/admin-cert --cacert /etc/elasticsearch/secret/admin-ca https://localhost:9200/$i -X DELETE; done     Explain allocation, frun from inside container</description>
      </item>
    
      <item>
        <title>Add SSH key to OpenShift 4</title>
        <link>https://briantward.github.io/add-sshkey-to-ocp4/</link>
        <pubDate>Sat, 19 Oct 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/add-sshkey-to-ocp4/</guid>
        <description>Add SSH key to OpenShift 4 To add an SSH key if one was not provided during installation, perform the following from an admin account such as system:admin:
 --- # oc debug node/&amp;lt;NODE_NAME&amp;gt; $ chroot /host $ mkdir /home/core/.ssh $ vi /home/core/.ssh/authorized_keys $ chown core:core -R /home/core/.ssh/ $ chmod 644 /home/core/.ssh/authorized_keys ---   Borrowed from Ryan Howe.
   </description>
      </item>
    
      <item>
        <title>Security Pipelines in OpenShift Container Platform</title>
        <link>https://briantward.github.io/security-pipelines/</link>
        <pubDate>Sat, 19 Oct 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/security-pipelines/</guid>
        <description>Security Pipelines in OpenShift Container Platform These are random notes for work in progress coded at https://github.com/briantward/container-pipelines/tree/parallel-spring-boot
 - two OCP clusters: nonprod (dev, test, etc) and prod (prod, stage) - allow nonprod cluster to continue pulling image updates automatically to registry - have separate registries between nonprod and prod - new builds only happen in nonprod, never in prod - existing dev pipeline must be aware of possible update to deployed container by alternative parallel pipeline route and keep base image in sync - new security pipeline, parallel to existing dev pipeline: -- poll for updates to images in nonprod cluster --- polling will check similar to what happens when the nonprod cluster syncs from RH Registry --- can we capture a log of the updates from RH registry and just act on them?</description>
      </item>
    
      <item>
        <title>OpenShift Authenticate all Namespaces to a Secured Registry</title>
        <link>https://briantward.github.io/openshift-auth-all-namespaces-to-a-secured-registry/</link>
        <pubDate>Fri, 24 May 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-auth-all-namespaces-to-a-secured-registry/</guid>
        <description>OpenShift Authenticate all Namespaces to a Secured Registry If your organization maintains or uses a third-party container registry requiring authentication, this article will help you setup one set of credentials in OpenShift for all your users.
 Implication: all users have access to these credentials. They should be read-only.
 Recently Red Hat launched a new Container Registry at registry.redhat.io requiring authenticated logins. When you install an OpenShift 3.10 or greater cluster, your default pull registry for the images and templates in the OpenShift namespace will use this registry.</description>
      </item>
    
      <item>
        <title>OpenShift JBoss EAP</title>
        <link>https://briantward.github.io/openshift-jboss-eap/</link>
        <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-jboss-eap/</guid>
        <description>OpenShift JBoss EAP Logging To update the server on-the-fly, if you are debugging and don’t want your change to be persistent:
 oc exec $POD_NAME -- /opt/eap/bin/jboss-cli.sh -c &amp;#34;/subsystem=logging/logger=org.keycloak:add(level=TRACE)&amp;#34;   Note that the above requires the server to already be booted. If you are troubleshooting the boot sequence you’ll need to update the standalone configuration. You may also want changes on a more permanent basis.
  Get the existing configuration:</description>
      </item>
    
      <item>
        <title>Configuring an External Heketi Prometheus Monitor on OpenShift</title>
        <link>https://briantward.github.io/openshift-prometheus-external-heketi/</link>
        <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-prometheus-external-heketi/</guid>
        <description>Configuring an External Heketi Prometheus Monitor on OpenShift Kudos goes to Ido Braunstain at devops.college for doing this on a raw Kubernetes cluster to monitor a GPU node. I adapted my information from his article to apply to monitoring both heketi and my external gluster nodes.
 Install the node-exporter on the external host First install docker to run the node-exporter container. You may want to consider configuring other docker options.</description>
      </item>
    
      <item>
        <title>OpenShift Remove Stuck ServiceInstance</title>
        <link>https://briantward.github.io/openshift-remove-stuck-serviceinstance/</link>
        <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-remove-stuck-serviceinstance/</guid>
        <description>OpenShift Remove Stuck ServiceInstance To delete a stuck serviceinstance where a project namespace no longer exists:
 $ oc get serviceinstance --all-namespaces -o wide NAMESPACE NAME CLASS PLAN STATUS AGE test cakephp-mysql-example-vfzkq ClusterServiceClass/cakephp-mysql-example default Failed 113d test cakephp-mysql-persistent-f75gl ClusterServiceClass/cakephp-mysql-persistent default Failed 113d webconsole-extensions httpd-example-6fxx5 ClusterServiceClass/httpd-example default DeprovisionCallFailed 10d    Create the project namespace again
$ oc new-project test     Now delete the serviceinstance
$ oc delete serviceinstance test -n cakephp-mysql-example-vfzkq     If that doesn’t delete it, then remove the finalizer</description>
      </item>
    
      <item>
        <title>OpenShift Reissue Certificate Manually</title>
        <link>https://briantward.github.io/openshift-reissue-certificate-manually/</link>
        <pubDate>Fri, 14 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-reissue-certificate-manually/</guid>
        <description>OpenShift Reissue Certificate Manually I recently ran the redeploy certificates playbook on my 3.11 cluster and found it broke apps that rely on the certificate signer ca, as it issues a new certificate signer ca but does not retrigger new certificates to be generated from it (at least not for all of the apps). In my case, it killed the latest Prometheus deployment and I got service unavailable messages from the router.</description>
      </item>
    
      <item>
        <title>Cgroups, cAdvisor, heapster, hawkular, and docker memory statistics in OpenShift</title>
        <link>https://briantward.github.io/memory-in-openshift/</link>
        <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/memory-in-openshift/</guid>
        <description>Cgroups, cAdvisor, heapster, hawkular, and docker memory statistics in OpenShift Work In Progress
 memory.usage_in_bytes # show current usage for memory (See 5.5 for details) memory.memsw.usage_in_bytes # show current usage for memory+Swap (See 5.5 for details)   memory.stat file includes following statistics
   per-memory cgroup local status cache - # of bytes of page cache memory. rss - # of bytes of anonymous and swap cache memory (includes transparent hugepages).</description>
      </item>
    
      <item>
        <title>OpenShift HTTPD loglevel</title>
        <link>https://briantward.github.io/openshift-httpd-loglevel/</link>
        <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-httpd-loglevel/</guid>
        <description>OpenShift HTTPD loglevel OpenShift comes with a container image packaged from this source. To make further configuration changes check the documentation here.
 https://github.com/sclorg/httpd-container
  Create a configmap to mount a log.conf file that contains your apache loglevel configuration. Be sure to update &amp;lt;PROJECT_NAMESPACE&amp;gt; below before running this command.
   echo &amp;#39;apiVersion: v1 data: log.conf: | LogLevel debug ErrorLog /dev/stdout TransferLog /dev/stdout&amp;#39; kind: ConfigMap metadata: name: logfile namespace: &amp;lt;PROJECT_NAMESPACE&amp;gt;&amp;#39; | oc create -f -    Update your deploymentConfig.</description>
      </item>
    
      <item>
        <title>OpenShift Project Backup and Migration Strategies</title>
        <link>https://briantward.github.io/openshift-backup-migration/</link>
        <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-backup-migration/</guid>
        <description>OpenShift Project Backup and Migration Strategies Work In Progress
 export
 secret bc is dc service route pv pvc
 remove (cinder) annotations for pv and pvc because it checks them
 replicate storage backend in new snapshot
   </description>
      </item>
    
      <item>
        <title>OpenShift Prometheus Node Exporter CrashLoop</title>
        <link>https://briantward.github.io/openshift-prometheus-node-exporter-crashloop/</link>
        <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-prometheus-node-exporter-crashloop/</guid>
        <description>OpenShift Prometheus Node Exporter CrashLoop Issue: A prometheus node exporter pod is stuck in a CrashLoopBackOff as a result of a failure to release the port 9100 bound by the previous instance before the next instance starts and attempts to reattach to it. This could potentially be resolved by changing the daemonset configuration (updateStrategy or terminationGracePeriodSeconds). Here, since we’ve only seen it once, we just kill the process holding the port open from the node itself.</description>
      </item>
    
      <item>
        <title>OpenShift Update Router Fix</title>
        <link>https://briantward.github.io/openshift-update-router-fix/</link>
        <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-update-router-fix/</guid>
        <description>OpenShift Update Router Fix Updating from v3.11.0 to v3.11.51 introduced a new volume mount on the router that did not previously exist (or maybe something wonky just happened in my cluster).
 Log message on router pod attempting to spin up. If you don’t have one attempting to spin up now (i.e. it failed a while back and just rolled back to the previous ReplicationController), delete the latest ReplicationController (not the one running the good pods!</description>
      </item>
    
      <item>
        <title>Migrate OpenShift PersistentVolumes from One Cluster to Another</title>
        <link>https://briantward.github.io/openshift-migrate-pv/</link>
        <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-migrate-pv/</guid>
        <description>Migrate OpenShift PersistentVolumes from One Cluster to Another Work In Progress!
 $ oc get pv mypvid -o yaml --export &amp;gt; mypv.yaml $ oc get pvc mypvcid -o yaml --export &amp;gt; mypvc.yaml   Remove all annotations and instance identifiers. If you leave them in place, you may get an error stating the PVC is lost.
 Verify that all SecurityContextContstraints are the same between each cluster and project environment, otherwise you may fail to gain ownership of the volume.</description>
      </item>
    
      <item>
        <title>OpenShift Web Console Extensions</title>
        <link>https://briantward.github.io/openshift-web-console-extensions/</link>
        <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-web-console-extensions/</guid>
        <description>OpenShift Web Console Extensions Testing Environment: OpenShift 3.11 Applicable Environment: OpenShift 3.9+
 As of OpenShift 3.9, the web console requires URL references rather than static content directories.[1]
 In OpenShift 3.7 and lower, you could mount static files from your masters through the master-config.yaml file.[2] Since this no longer applies, we have to provide our own webserver with the content, to be referenced by the web console pod remotely. I checked for ways to mount static files to the web console pod; however, in the new design there is no static directory location within the pod itself from which it could reference such files.</description>
      </item>
    
      <item>
        <title>Automatically Update Red Hat Container Images on OpenShift 3.11</title>
        <link>https://briantward.github.io/sync-redhat-images-on-openshift-311/</link>
        <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/sync-redhat-images-on-openshift-311/</guid>
        <description>Automatically Update Red Hat Container Images on OpenShift 3.11 OpenShift manages container images using a registry. This is the place where it caches upstream container images and stores the images from your own builds as well. Each build or container image correlates to an ImageStream, which is an object that defines any number of related images by tags. For example, one specific version of a Ruby container might be v2.5-22, but you can have one ImageStream definition that holds ruby tags and correlating images for v2.</description>
      </item>
    
      <item>
        <title>OpenShift Image Management</title>
        <link>https://briantward.github.io/openshift-image-management/</link>
        <pubDate>Wed, 15 Aug 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-image-management/</guid>
        <description>OpenShift Image Management $ oc project openshift Now using project &amp;#34;openshift&amp;#34; on server &amp;#34;https://openshift.example.com:8443&amp;#34;. $ oc get is | grep php NAME DOCKER REPO TAGS UPDATED php docker-registry.default.svc:5000/openshift/php 7.1,latest,5.6 + 2 more... 11 days ago   $ oc import-image registry.access.redhat.com/rhscl/php-70-rhel7:7.0-17 --confirm The import completed successfully. Name:	php-70-rhel7 Namespace:	openshift Created:	Less than a second ago Labels:	&amp;lt;none&amp;gt; Annotations:	openshift.io/image.dockerRepositoryCheck=2018-08-15T18:38:10Z Docker Pull Spec:	docker-registry.default.svc:5000/openshift/php-70-rhel7 Image Lookup:	local=false Unique Images:	1 Tags:	1 7.</description>
      </item>
    
      <item>
        <title>Import Images with dockerImageRepository</title>
        <link>https://briantward.github.io/import-images-with-dockerimagerepository/</link>
        <pubDate>Wed, 25 Jul 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/import-images-with-dockerimagerepository/</guid>
        <description>Import Images with dockerImageRepository $ echo &amp;#39;apiVersion: v1 &amp;gt; kind: ImageStream &amp;gt; metadata: &amp;gt; creationTimestamp: null &amp;gt; generation: 2 &amp;gt; labels: &amp;gt; build: is-test &amp;gt; name: jenkins-slave-base-centos7 &amp;gt; spec: &amp;gt; dockerImageRepository: docker.io/openshift/jenkins-slave-base-centos7&amp;#39; | oc apply -f- imagestream &amp;#34;jenkins-slave-base-centos7&amp;#34; created [esauer@localhost image-scanning]$ oc export is jenkins-slave-base-centos7 apiVersion: v1 kind: ImageStream metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {&amp;#34;apiVersion&amp;#34;:&amp;#34;v1&amp;#34;,&amp;#34;kind&amp;#34;:&amp;#34;ImageStream&amp;#34;,&amp;#34;metadata&amp;#34;:{&amp;#34;annotations&amp;#34;:{},&amp;#34;creationTimestamp&amp;#34;:null,&amp;#34;generation&amp;#34;:2,&amp;#34;labels&amp;#34;:{&amp;#34;build&amp;#34;:&amp;#34;is-test&amp;#34;},&amp;#34;name&amp;#34;:&amp;#34;jenkins-slave-base-centos7&amp;#34;,&amp;#34;namespace&amp;#34;:&amp;#34;sbx-esauer&amp;#34;},&amp;#34;spec&amp;#34;:{&amp;#34;dockerImageRepository&amp;#34;:&amp;#34;docker.io/openshift/jenkins-slave-base-centos7&amp;#34;}} openshift.io/image.dockerRepositoryCheck: 2018-07-25T13:47:59Z creationTimestamp: null generation: 2 labels: build: is-test name: jenkins-slave-base-centos7 spec: dockerImageRepository: docker.io/openshift/jenkins-slave-base-centos7 lookupPolicy: local: false tags: - annotations: null from: kind: DockerImage name: docker.</description>
      </item>
    
      <item>
        <title>Sync Red Hat Container Images on OpenShift 3.9</title>
        <link>https://briantward.github.io/sync-redhat-images-on-openshift-39/</link>
        <pubDate>Wed, 25 Jul 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/sync-redhat-images-on-openshift-39/</guid>
        <description>Sync Red Hat Container Images on OpenShift 3.9 If using the default Advanced Installer, and setting the flag to deploy openshift_install_examples [1] in your cluster (or using the default which is true), you will find that the ansible installer adds some nice stuff to your local registry from the openshift_examples [2] folder.
 $ oc get is -n openshift NAME DOCKER REPO TAGS UPDATED imagestreams/eap71-openshift docker-registry.default.svc:5000/openshift/eap71-openshift latest 3 months ago imagestreams/httpd docker-registry.</description>
      </item>
    
      <item>
        <title>Let&#39;s Encrypt on OpenShift</title>
        <link>https://briantward.github.io/lets-encrypt-on-openshift/</link>
        <pubDate>Mon, 21 May 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/lets-encrypt-on-openshift/</guid>
        <description>Let’s Encrypt on OpenShift Updated OS on load balancer
 # wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # yum install epel-release-latest-7.noarch.rpm   Uses python:
   https://www.redpill-linpro.com/sysadvent/2017/12/15/letsencrypt-on-openshift.html
  https://certbot.eff.org/about/
   Uses bash: - https://blog.openshift.com/lets-encrypt-acme-v2-api/ - https://github.com/Neilpang/acme.sh#currently-acmesh-supports
   https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live/55579
  https://www.namecheap.com/
   Other:
   https://github.com/certbot/certbot/issues/5074
  https://github.com/freeipa/freeipa-letsencrypt
  https://github.com/antevens/letsencrypt-freeipa
  https://certbot.eff.org/docs/using.html#dns-plugins
     </description>
      </item>
    
      <item>
        <title>Hugo Blog on OpenShift</title>
        <link>https://briantward.github.io/hugo-blog-on-openshift/</link>
        <pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/hugo-blog-on-openshift/</guid>
        <description>Create a Dockerfile that adds all the dependencies your environment needs. A lot of the work is in this Dockerfile. I used asciidocs, so I needed an implementation of either asciidoc or asciidoctor, the two options that Hugo provides integration with. I chose asciidoctor.
   $ cat Dockerfile FROM centos:centos7 COPY . /opt/blog RUN cd /opt \ &amp;amp;&amp;amp; curl -O -J -L https://github.com/gohugoio/hugo/releases/download/v0.40.3/hugo_0.40.3_Linux-64bit.tar.gz \ &amp;amp;&amp;amp; tar -xf hugo_0.</description>
      </item>
    
      <item>
        <title>Authenticate Openshift Console with RH-SSO</title>
        <link>https://briantward.github.io/rh-sso-authenticating-openshift-console/</link>
        <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
        <guid>https://briantward.github.io/rh-sso-authenticating-openshift-console/</guid>
        <description>Authenticate Openshift Console with RH-SSO Install based on OpenShift 3.7. Will probably work on other similar versions.
 Be aware of default permissions on your platform.
   https://docs.openshift.com/container-platform/3.7/admin_solutions/user_role_mgmt.html#determine-default-user-roles
   Be aware of the implications of using Google as an Identity Broker.
 Master and Node Configuration:
   https://docs.openshift.com/container-platform/3.7/admin_solutions/master_node_config.htmli
   Here is a great step-by-step example workflow in Red Hat official documentation:
   https://access.</description>
      </item>
    
      <item>
        <title>Configure Google OIDC</title>
        <link>https://briantward.github.io/configure-google-oidc/</link>
        <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
        <guid>https://briantward.github.io/configure-google-oidc/</guid>
        <description>Configure Google OIDC https://console.cloud.google.com
 Since this topic has been well handled by the Keycloak team, I’m adding there documentation here.
   https://www.keycloak.org/docs/3.4/server_admin/index.html#google
     </description>
      </item>
    
      <item>
        <title>Configure RH-SSO to Identity Broker with Google by Hosted Domain</title>
        <link>https://briantward.github.io/rh-sso-google-oidc/</link>
        <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
        <guid>https://briantward.github.io/rh-sso-google-oidc/</guid>
        <description>Configure RH-SSO to Identity Broker with Google by Hosted Domain In a browser, go to your RH-SSO admin console. This should be your hostname with context /auth/admin, e.g. https://sso.apps.example.com/auth/admin/
 Login with the username and password you set in your template for fields:
 SSO_ADMIN_USERNAME=admin SSO_ADMIN_PASSWORD=redacted   First, create a new realm for your OpenShift users. Out of the box, RH-SSO is configured with a master realm, but this should only be used for administration of the RH-SSO server itself.</description>
      </item>
    
      <item>
        <title>Install a Quickstart App on Openshift Authenticated by RH-SSO</title>
        <link>https://briantward.github.io/rh-sso-openshift-quickstart/</link>
        <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
        <guid>https://briantward.github.io/rh-sso-openshift-quickstart/</guid>
        <description>Install a Quickstart App on Openshift Authenticated by RH-SSO You will need to build and run an app and configure the client in RH-SSO.
 $ oc get template -n openshift | grep sso eap64-sso-s2i An example EAP 6 Single Sign-On application. For more information about using... 44 (19 blank) 8 eap70-sso-s2i An example EAP 7 Single Sign-On application. For more information about using... 44 (19 blank) 8 eap71-sso-s2i An example EAP 7 Single Sign-On application.</description>
      </item>
    
      <item>
        <title>Install RH-SSO on Openshift</title>
        <link>https://briantward.github.io/rh-sso-on-openshift/</link>
        <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
        <guid>https://briantward.github.io/rh-sso-on-openshift/</guid>
        <description>Install RH-SSO on Openshift First create a new project to keep RH-SSO work clean and easily delete it if necessary.
 oc new-project sso   I started by exporting the existing RH-SSO persistent depoyment template. This builds up one RH-SSO app with one postgres persistent database by default. You’ve got lots of other choices.
 $ oc get template -n openshift | grep sso eap64-sso-s2i An example EAP 6 Single Sign-On application.</description>
      </item>
    
      <item>
        <title>RH-SSO Authentication on Openshift Series</title>
        <link>https://briantward.github.io/rh-sso-on-ocp-series/</link>
        <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
        <guid>https://briantward.github.io/rh-sso-on-ocp-series/</guid>
        <description>RH-SSO Authentication on Openshift Series This is a multi-part series on RH-SSO.
  Install RH-SSO on Openshift
  Install a Quickstart App on Openshift Authenticated by RH-SSO
  Configure Google OIDC
  Configure RH-SSO to Identity Broker with Google by Hosted Domain
  Authenticate Openshift Console with RH-SSO
     </description>
      </item>
    
      <item>
        <title>Custom OpenShift 4 Ingress Router</title>
        <link>https://briantward.github.io/custom-openshift-4-ingress-router/</link>
        <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/custom-openshift-4-ingress-router/</guid>
        <description>Custom OpenShift 4 Ingress Router This solution is incomplete and is mostly scratch notes. This design is NOT supported by Red Hat! Use at your own risk.
  Create custom HAProxy Template
  Create a ConfigMap From the HAProxy Template (these are incomplete Ansible tasks/plays)
   - name: slurp slurp: src: template file register: haproxy-template - name: k8s: state: present namespace: openshift-ingress definition: apiVersion: v1 kind: ConfigMap metadata: name: haproxy-template data: haproxy-config.</description>
      </item>
    
  </channel>
</rss>

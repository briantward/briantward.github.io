<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>openshift on Everyday Linux</title>
    <link>https://briantward.github.io/categories/openshift/</link>
    <description>Recent content in openshift on Everyday Linux</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2019 00:00:00 +0000</lastBuildDate>
    
      <atom:link href="https://briantward.github.io/categories/openshift/index.xml" rel="self" type="application/rss+xml" />
    
    
      <item>
        <title>OpenShift Authenticate all Namespaces to a Secured Registry</title>
        <link>https://briantward.github.io/openshift-auth-all-namespaces-to-a-secured-registry/</link>
        <pubDate>Fri, 24 May 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-auth-all-namespaces-to-a-secured-registry/</guid>
        <description>OpenShift Authenticate all Namespaces to a Secured Registry If your organization maintains or uses a third-party container registry requiring authentication, this article will help you setup one set of credentials in OpenShift for all your users.
 Implication: all users have access to these credentials. They should be read-only.
 Recently Red Hat launched a new Container Registry at registry.redhat.io requiring authenticated logins. When you install an OpenShift 3.10 or greater cluster, your default pull registry for the images and templates in the OpenShift namespace will use this registry.</description>
      </item>
    
      <item>
        <title>OpenShift JBoss EAP</title>
        <link>https://briantward.github.io/openshift-jboss-eap/</link>
        <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-jboss-eap/</guid>
        <description>OpenShift JBoss EAP Logging To update the server on-the-fly, if you are debugging and don&amp;#8217;t want your change to be persistent:
 oc exec $POD_NAME -- /opt/eap/bin/jboss-cli.sh -c &#34;/subsystem=logging/logger=org.keycloak:add(level=TRACE)&#34;   Note that the above requires the server to already be booted. If you are troubleshooting the boot sequence you&amp;#8217;ll need to update the standalone configuration. You may also want changes on a more permanent basis.
  Get the existing configuration:</description>
      </item>
    
      <item>
        <title>Configuring an External Heketi Prometheus Monitor on OpenShift</title>
        <link>https://briantward.github.io/openshift-prometheus-external-heketi/</link>
        <pubDate>Tue, 15 Jan 2019 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-prometheus-external-heketi/</guid>
        <description>Configuring an External Heketi Prometheus Monitor on OpenShift Kudos goes to Ido Braunstain at devops.college for doing this on a raw Kubernetes cluster to monitor a GPU node. I adapted my information from his article to apply to monitoring both heketi and my external gluster nodes.
 Install the node-exporter on the external host First install docker to run the node-exporter container. You may want to consider configuring other docker options.</description>
      </item>
    
      <item>
        <title>OpenShift Remove Stuck ServiceInstance</title>
        <link>https://briantward.github.io/openshift-remove-stuck-serviceinstance/</link>
        <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-remove-stuck-serviceinstance/</guid>
        <description>OpenShift Remove Stuck ServiceInstance To delete a stuck serviceinstance where a project namespace no longer exists:
 $ oc get serviceinstance --all-namespaces -o wide NAMESPACE NAME CLASS PLAN STATUS AGE test cakephp-mysql-example-vfzkq ClusterServiceClass/cakephp-mysql-example default Failed 113d test cakephp-mysql-persistent-f75gl ClusterServiceClass/cakephp-mysql-persistent default Failed 113d webconsole-extensions httpd-example-6fxx5 ClusterServiceClass/httpd-example default DeprovisionCallFailed 10d    Create the project namespace again
$ oc new-project test     Now delete the serviceinstance
$ oc delete serviceinstance test -n cakephp-mysql-example-vfzkq     If that doesn&amp;#8217;t delete it, then remove the finalizer</description>
      </item>
    
      <item>
        <title>OpenShift Reissue Certificate Manually</title>
        <link>https://briantward.github.io/openshift-reissue-certificate-manually/</link>
        <pubDate>Fri, 14 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-reissue-certificate-manually/</guid>
        <description>OpenShift Reissue Certificate Manually I recently ran the redeploy certificates playbook on my 3.11 cluster and found it broke apps that rely on the certificate signer ca, as it issues a new certificate signer ca but does not retrigger new certificates to be generated from it (at least not for all of the apps). In my case, it killed the latest Prometheus deployment and I got service unavailable messages from the router.</description>
      </item>
    
      <item>
        <title>Cgroups, cAdvisor, heapster, hawkular, and docker memory statistics in OpenShift</title>
        <link>https://briantward.github.io/memory-in-openshift/</link>
        <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/memory-in-openshift/</guid>
        <description>Work In Progress
   Linux top
 VIRT RES SHR free used buff/cache avail Mem
   cgroups Cgroups reports a bunch of memory stats:
 &amp;lt;examples&amp;gt;
 The cgroups kernel team still thinks the best calculation for memory usage would be RSS+CACHE(+SWAP) values in memory.stat [3]. Also note that RSS here is not the same as RES on top as explained at the bottom of section 5.</description>
      </item>
    
      <item>
        <title>OpenShift HTTPD loglevel</title>
        <link>https://briantward.github.io/openshift-httpd-loglevel/</link>
        <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-httpd-loglevel/</guid>
        <description>OpenShift HTTPD loglevel OpenShift comes with a container image packaged from this source. To make further configuration changes check the documentation here.
 https://github.com/sclorg/httpd-container
  Create a configmap to mount a log.conf file that contains your apache loglevel configuration. Be sure to update &amp;lt;PROJECT_NAMESPACE&amp;gt; below before running this command.
   echo &#39;apiVersion: v1 data: log.conf: | LogLevel debug ErrorLog /dev/stdout TransferLog /dev/stdout&#39; kind: ConfigMap metadata: name: logfile namespace: &amp;lt;PROJECT_NAMESPACE&amp;gt;&#39; | oc create -f -    Update your deploymentConfig.</description>
      </item>
    
      <item>
        <title>OpenShift Project Backup and Migration Strategies</title>
        <link>https://briantward.github.io/openshift-backup-migration/</link>
        <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-backup-migration/</guid>
        <description>OpenShift Project Backup and Migration Strategies Work In Progress
 export
 secret bc is dc service route pv pvc
 remove (cinder) annotations for pv and pvc because it checks them
 replicate storage backend in new snapshot
   </description>
      </item>
    
      <item>
        <title>OpenShift Prometheus Node Exporter CrashLoop</title>
        <link>https://briantward.github.io/openshift-prometheus-node-exporter-crashloop/</link>
        <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-prometheus-node-exporter-crashloop/</guid>
        <description>OpenShift Prometheus Node Exporter CrashLoop Issue: A prometheus node exporter pod is stuck in a CrashLoopBackOff as a result of a failure to release the port 9100 bound by the previous instance before the next instance starts and attempts to reattach to it. This could potentially be resolved by changing the daemonset configuration (updateStrategy or terminationGracePeriodSeconds). Here, since we&amp;#8217;ve only seen it once, we just kill the process holding the port open from the node itself.</description>
      </item>
    
      <item>
        <title>OpenShift Update Router Fix</title>
        <link>https://briantward.github.io/openshift-update-router-fix/</link>
        <pubDate>Thu, 13 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-update-router-fix/</guid>
        <description>OpenShift Update Router Fix Updating from v3.11.0 to v3.11.51 introduced a new volume mount on the router that did not previously exist (or maybe something wonky just happened in my cluster).
 Log message on router pod attempting to spin up. If you don&amp;#8217;t have one attempting to spin up now (i.e. it failed a while back and just rolled back to the previous ReplicationController), delete the latest ReplicationController (not the one running the good pods!</description>
      </item>
    
      <item>
        <title>Migrate OpenShift PersistentVolumes from One Cluster to Another</title>
        <link>https://briantward.github.io/openshift-migrate-pv/</link>
        <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-migrate-pv/</guid>
        <description>Migrate OpenShift PersistentVolumes from One Cluster to Another Work In Progress!
 $ oc get pv mypvid -o yaml --export &amp;gt; mypv.yaml $ oc get pvc mypvcid -o yaml --export &amp;gt; mypvc.yaml   Remove all annotations and instance identifiers. If you leave them in place, you may get an error stating the PVC is lost.
 Verify that all SecurityContextContstraints are the same between each cluster and project environment, otherwise you may fail to gain ownership of the volume.</description>
      </item>
    
      <item>
        <title>OpenShift Web Console Extensions</title>
        <link>https://briantward.github.io/openshift-web-console-extensions/</link>
        <pubDate>Tue, 04 Dec 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-web-console-extensions/</guid>
        <description>OpenShift Web Console Extensions Testing Environment: OpenShift 3.11
Applicable Environment: OpenShift 3.9+
 As of OpenShift 3.9, the web console requires URL references rather than static content directories.[1]
 In OpenShift 3.7 and lower, you could mount static files from your masters through the master-config.yaml file.[2] Since this no longer applies, we have to provide our own webserver with the content, to be referenced by the web console pod remotely. I checked for ways to mount static files to the web console pod; however, in the new design there is no static directory location within the pod itself from which it could reference such files.</description>
      </item>
    
      <item>
        <title>Automatically Update Red Hat Container Images on OpenShift 3.11</title>
        <link>https://briantward.github.io/sync-redhat-images-on-openshift-311/</link>
        <pubDate>Thu, 01 Nov 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/sync-redhat-images-on-openshift-311/</guid>
        <description>Automatically Update Red Hat Container Images on OpenShift 3.11 OpenShift manages container images using a registry. This is the place where it caches upstream container images and stores the images from your own builds as well. Each build or container image correlates to an ImageStream, which is an object that defines any number of related images by tags. For example, one specific version of a Ruby container might be v2.5-22, but you can have one ImageStream definition that holds ruby tags and correlating images for v2.</description>
      </item>
    
      <item>
        <title>OpenShift Image Management</title>
        <link>https://briantward.github.io/openshift-image-management/</link>
        <pubDate>Wed, 15 Aug 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/openshift-image-management/</guid>
        <description>OpenShift Image Management $ oc project openshift Now using project &#34;openshift&#34; on server &#34;https://openshift.example.com:8443&#34;. $ oc get is | grep php NAME DOCKER REPO TAGS UPDATED php docker-registry.default.svc:5000/openshift/php 7.1,latest,5.6 + 2 more... 11 days ago   $ oc import-image registry.access.redhat.com/rhscl/php-70-rhel7:7.0-17 --confirm The import completed successfully. Name:	php-70-rhel7 Namespace:	openshift Created:	Less than a second ago Labels:	&amp;lt;none&amp;gt; Annotations:	openshift.io/image.dockerRepositoryCheck=2018-08-15T18:38:10Z Docker Pull Spec:	docker-registry.default.svc:5000/openshift/php-70-rhel7 Image Lookup:	local=false Unique Images:	1 Tags:	1 7.</description>
      </item>
    
      <item>
        <title>Import Images with dockerImageRepository</title>
        <link>https://briantward.github.io/import-images-with-dockerimagerepository/</link>
        <pubDate>Wed, 25 Jul 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/import-images-with-dockerimagerepository/</guid>
        <description>Import Images with dockerImageRepository $ echo &#39;apiVersion: v1 &amp;gt; kind: ImageStream &amp;gt; metadata: &amp;gt; creationTimestamp: null &amp;gt; generation: 2 &amp;gt; labels: &amp;gt; build: is-test &amp;gt; name: jenkins-slave-base-centos7 &amp;gt; spec: &amp;gt; dockerImageRepository: docker.io/openshift/jenkins-slave-base-centos7&#39; | oc apply -f- imagestream &#34;jenkins-slave-base-centos7&#34; created [esauer@localhost image-scanning]$ oc export is jenkins-slave-base-centos7 apiVersion: v1 kind: ImageStream metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {&#34;apiVersion&#34;:&#34;v1&#34;,&#34;kind&#34;:&#34;ImageStream&#34;,&#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;creationTimestamp&#34;:null,&#34;generation&#34;:2,&#34;labels&#34;:{&#34;build&#34;:&#34;is-test&#34;},&#34;name&#34;:&#34;jenkins-slave-base-centos7&#34;,&#34;namespace&#34;:&#34;sbx-esauer&#34;},&#34;spec&#34;:{&#34;dockerImageRepository&#34;:&#34;docker.io/openshift/jenkins-slave-base-centos7&#34;}} openshift.io/image.dockerRepositoryCheck: 2018-07-25T13:47:59Z creationTimestamp: null generation: 2 labels: build: is-test name: jenkins-slave-base-centos7 spec: dockerImageRepository: docker.io/openshift/jenkins-slave-base-centos7 lookupPolicy: local: false tags: - annotations: null from: kind: DockerImage name: docker.</description>
      </item>
    
      <item>
        <title>Sync Red Hat Container Images on OpenShift 3.9</title>
        <link>https://briantward.github.io/sync-redhat-images-on-openshift-39/</link>
        <pubDate>Wed, 25 Jul 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/sync-redhat-images-on-openshift-39/</guid>
        <description>Sync Red Hat Container Images on OpenShift 3.9 If using the default Advanced Installer, and setting the flag to deploy openshift_install_examples [1] in your cluster (or using the default which is true), you will find that the ansible installer adds some nice stuff to your local registry from the openshift_examples [2] folder.
 $ oc get is -n openshift NAME DOCKER REPO TAGS UPDATED imagestreams/eap71-openshift docker-registry.default.svc:5000/openshift/eap71-openshift latest 3 months ago imagestreams/httpd docker-registry.</description>
      </item>
    
      <item>
        <title>Let&#39;s Encrypt on OpenShift</title>
        <link>https://briantward.github.io/lets-encrypt-on-openshift/</link>
        <pubDate>Mon, 21 May 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/lets-encrypt-on-openshift/</guid>
        <description>Let&amp;#8217;s Encrypt on OpenShift Updated OS on load balancer
 # wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # yum install epel-release-latest-7.noarch.rpm   Uses python:
   https://www.redpill-linpro.com/sysadvent/2017/12/15/letsencrypt-on-openshift.html
  https://certbot.eff.org/about/
   Uses bash: - https://blog.openshift.com/lets-encrypt-acme-v2-api/ - https://github.com/Neilpang/acme.sh#currently-acmesh-supports
   https://community.letsencrypt.org/t/acme-v2-and-wildcard-certificate-support-is-live/55579
  https://www.namecheap.com/
   Other:
   https://github.com/certbot/certbot/issues/5074
  https://github.com/freeipa/freeipa-letsencrypt
  https://github.com/antevens/letsencrypt-freeipa
  https://certbot.eff.org/docs/using.html#dns-plugins
     </description>
      </item>
    
      <item>
        <title>Hugo Blog on OpenShift</title>
        <link>https://briantward.github.io/hugo-blog-on-openshift/</link>
        <pubDate>Fri, 11 May 2018 00:00:00 +0000</pubDate>
        <guid>https://briantward.github.io/hugo-blog-on-openshift/</guid>
        <description>Create a Dockerfile that adds all the dependencies your environment needs. A lot of the work is in this Dockerfile. I used asciidocs, so I needed an implementation of either asciidoc or asciidoctor, the two options that Hugo provides integration with. I chose asciidoctor.
   $ cat Dockerfile FROM centos:centos7 COPY . /opt/blog RUN cd /opt \ &amp;amp;&amp;amp; curl -O -J -L https://github.com/gohugoio/hugo/releases/download/v0.40.3/hugo_0.40.3_Linux-64bit.tar.gz \ &amp;amp;&amp;amp; tar -xf hugo_0.</description>
      </item>
    
      <item>
        <title>Authenticate Openshift Console with RH-SSO</title>
        <link>https://briantward.github.io/rh-sso-authenticating-openshift-console/</link>
        <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
        <guid>https://briantward.github.io/rh-sso-authenticating-openshift-console/</guid>
        <description>Authenticate Openshift Console with RH-SSO Install based on OpenShift 3.7. Will probably work on other similar versions.
 Be aware of default permissions on your platform.
   https://docs.openshift.com/container-platform/3.7/admin_solutions/user_role_mgmt.html#determine-default-user-roles
   Be aware of the implications of using Google as an Identity Broker.
 Master and Node Configuration:
   https://docs.openshift.com/container-platform/3.7/admin_solutions/master_node_config.htmli
   Here is a great step-by-step example workflow in Red Hat official documentation:
   https://access.</description>
      </item>
    
      <item>
        <title>Configure Google OIDC</title>
        <link>https://briantward.github.io/configure-google-oidc/</link>
        <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
        <guid>https://briantward.github.io/configure-google-oidc/</guid>
        <description>Configure Google OIDC https://console.cloud.google.com
 Since this topic has been well handled by the Keycloak team, I&amp;#8217;m adding there documentation here.
   https://www.keycloak.org/docs/3.4/server_admin/index.html#google
     </description>
      </item>
    
      <item>
        <title>Configure RH-SSO to Identity Broker with Google by Hosted Domain</title>
        <link>https://briantward.github.io/rh-sso-google-oidc/</link>
        <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
        <guid>https://briantward.github.io/rh-sso-google-oidc/</guid>
        <description>Configure RH-SSO to Identity Broker with Google by Hosted Domain In a browser, go to your RH-SSO admin console. This should be your hostname with context /auth/admin, e.g. https://sso.apps.example.com/auth/admin/
 Login with the username and password you set in your template for fields:
 SSO_ADMIN_USERNAME=admin SSO_ADMIN_PASSWORD=redacted   First, create a new realm for your OpenShift users. Out of the box, RH-SSO is configured with a master realm, but this should only be used for administration of the RH-SSO server itself.</description>
      </item>
    
      <item>
        <title>Install RH-SSO on Openshift</title>
        <link>https://briantward.github.io/rh-sso-on-openshift/</link>
        <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
        <guid>https://briantward.github.io/rh-sso-on-openshift/</guid>
        <description>Install RH-SSO on Openshift First create a new project to keep RH-SSO work clean and easily delete it if necessary.
 oc new-project sso   I started by exporting the existing RH-SSO persistent depoyment template. This builds up one RH-SSO app with one postgres persistent database by default. You&amp;#8217;ve got lots of other choices.
 $ oc get template -n openshift | grep sso eap64-sso-s2i An example EAP 6 Single Sign-On application.</description>
      </item>
    
      <item>
        <title>Install a Quickstart App on Openshift Authenticated by RH-SSO</title>
        <link>https://briantward.github.io/rh-sso-openshift-quickstart/</link>
        <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
        <guid>https://briantward.github.io/rh-sso-openshift-quickstart/</guid>
        <description>Install a Quickstart App on Openshift Authenticated by RH-SSO You will need to build and run an app and configure the client in RH-SSO.
 $ oc get template -n openshift | grep sso eap64-sso-s2i An example EAP 6 Single Sign-On application. For more information about using... 44 (19 blank) 8 eap70-sso-s2i An example EAP 7 Single Sign-On application. For more information about using... 44 (19 blank) 8 eap71-sso-s2i An example EAP 7 Single Sign-On application.</description>
      </item>
    
      <item>
        <title>RH-SSO Authentication on Openshift Series</title>
        <link>https://briantward.github.io/rh-sso-on-ocp-series/</link>
        <pubDate>Wed, 09 May 2018 12:00:00 -0400</pubDate>
        <guid>https://briantward.github.io/rh-sso-on-ocp-series/</guid>
        <description>RH-SSO Authentication on Openshift Series This is a multi-part series on RH-SSO.
  Install RH-SSO on Openshift
  Install a Quickstart App on Openshift Authenticated by RH-SSO
  Configure Google OIDC
  Configure RH-SSO to Identity Broker with Google by Hosted Domain
  Authenticate Openshift Console with RH-SSO
     </description>
      </item>
    
  </channel>
</rss>
